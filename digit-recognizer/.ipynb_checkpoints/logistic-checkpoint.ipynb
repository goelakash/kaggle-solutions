{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as data\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data size: 42000\n",
      "Test data size: 28000\n"
     ]
    }
   ],
   "source": [
    "print(\"Train data size: {}\".format(len(train_data)))\n",
    "print(\"Test data size: {}\".format(len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "784\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x_train = torch.tensor(train_data.values[:, 1:])\n",
    "train_set = torch.stack([data for i,data in enumerate(x_train) if (i+1)%5 != 0]) #because its a list of tensors\n",
    "val_set = torch.stack([data for i,data in enumerate(x_train) if (i+1)%5 == 0])\n",
    "# print(len(train_set))\n",
    "# print(len(val_set))\n",
    "\n",
    "y_train = torch.tensor(train_data.values[:,0])\n",
    "train_labels = [data for i,data in enumerate(y_train) if (i+1)%5 != 0]\n",
    "# print(train_labels[:5])\n",
    "val_labels = [data for i,data in enumerate(y_train) if (i+1)%5 == 0]\n",
    "# print(val_labels[:5])\n",
    "# print(len(train_labels))\n",
    "# print(len(val_labels))\n",
    "\n",
    "x_test = torch.tensor(test_data.values)\n",
    "\n",
    "train_set_loader = data.DataLoader(dataset=train_set.float(), batch_size = 100, shuffle=True)\n",
    "val_set_loader = data.DataLoader(dataset=val_set.float(), batch_size = 100, shuffle=True)\n",
    "\n",
    "\n",
    "N_CLASSES = 10 # 0-9 digits\n",
    "INPUT_SIZE = train_set.shape[1]\n",
    "print(INPUT_SIZE)\n",
    "LEARNING_RATE = 0.001\n",
    "N_EPOCHS = 50\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simple logistic classifier\n",
    "classifer = nn.Linear(INPUT_SIZE, N_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(classifer.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_steps = len(train_set_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Step [100/336], Loss: 716.3787\n",
      "Epoch [1/50], Step [200/336], Loss: 534.5586\n",
      "Epoch [1/50], Step [300/336], Loss: 552.3190\n",
      "0\n",
      "Epoch [2/50], Step [100/336], Loss: 811.3234\n",
      "Epoch [2/50], Step [200/336], Loss: 757.1703\n",
      "Epoch [2/50], Step [300/336], Loss: 579.3011\n",
      "1\n",
      "Epoch [3/50], Step [100/336], Loss: 599.4883\n",
      "Epoch [3/50], Step [200/336], Loss: 638.6340\n",
      "Epoch [3/50], Step [300/336], Loss: 429.2354\n",
      "2\n",
      "Epoch [4/50], Step [100/336], Loss: 613.6575\n",
      "Epoch [4/50], Step [200/336], Loss: 506.8609\n",
      "Epoch [4/50], Step [300/336], Loss: 549.6960\n",
      "3\n",
      "Epoch [5/50], Step [100/336], Loss: 618.2960\n",
      "Epoch [5/50], Step [200/336], Loss: 708.1721\n",
      "Epoch [5/50], Step [300/336], Loss: 760.3423\n",
      "4\n",
      "Epoch [6/50], Step [100/336], Loss: 573.9576\n",
      "Epoch [6/50], Step [200/336], Loss: 493.8232\n",
      "Epoch [6/50], Step [300/336], Loss: 670.9511\n",
      "5\n",
      "Epoch [7/50], Step [100/336], Loss: 593.3965\n",
      "Epoch [7/50], Step [200/336], Loss: 465.7754\n",
      "Epoch [7/50], Step [300/336], Loss: 577.9171\n",
      "6\n",
      "Epoch [8/50], Step [100/336], Loss: 536.4161\n",
      "Epoch [8/50], Step [200/336], Loss: 772.8450\n",
      "Epoch [8/50], Step [300/336], Loss: 562.0074\n",
      "7\n",
      "Epoch [9/50], Step [100/336], Loss: 648.8852\n",
      "Epoch [9/50], Step [200/336], Loss: 528.0445\n",
      "Epoch [9/50], Step [300/336], Loss: 681.4296\n",
      "8\n",
      "Epoch [10/50], Step [100/336], Loss: 705.6885\n",
      "Epoch [10/50], Step [200/336], Loss: 386.2614\n",
      "Epoch [10/50], Step [300/336], Loss: 486.4111\n",
      "9\n",
      "Epoch [11/50], Step [100/336], Loss: 594.6826\n",
      "Epoch [11/50], Step [200/336], Loss: 649.3694\n",
      "Epoch [11/50], Step [300/336], Loss: 497.9701\n",
      "10\n",
      "Epoch [12/50], Step [100/336], Loss: 589.5962\n",
      "Epoch [12/50], Step [200/336], Loss: 720.5557\n",
      "Epoch [12/50], Step [300/336], Loss: 488.0815\n",
      "11\n",
      "Epoch [13/50], Step [100/336], Loss: 402.2713\n",
      "Epoch [13/50], Step [200/336], Loss: 670.0670\n",
      "Epoch [13/50], Step [300/336], Loss: 733.6213\n",
      "12\n",
      "Epoch [14/50], Step [100/336], Loss: 863.4103\n",
      "Epoch [14/50], Step [200/336], Loss: 662.2204\n",
      "Epoch [14/50], Step [300/336], Loss: 697.0328\n",
      "13\n",
      "Epoch [15/50], Step [100/336], Loss: 886.8702\n",
      "Epoch [15/50], Step [200/336], Loss: 544.0623\n",
      "Epoch [15/50], Step [300/336], Loss: 481.1564\n",
      "14\n",
      "Epoch [16/50], Step [100/336], Loss: 596.7675\n",
      "Epoch [16/50], Step [200/336], Loss: 625.3853\n",
      "Epoch [16/50], Step [300/336], Loss: 733.3663\n",
      "15\n",
      "Epoch [17/50], Step [100/336], Loss: 583.5310\n",
      "Epoch [17/50], Step [200/336], Loss: 848.3908\n",
      "Epoch [17/50], Step [300/336], Loss: 641.1063\n",
      "16\n",
      "Epoch [18/50], Step [100/336], Loss: 527.6190\n",
      "Epoch [18/50], Step [200/336], Loss: 681.3914\n",
      "Epoch [18/50], Step [300/336], Loss: 346.2778\n",
      "17\n",
      "Epoch [19/50], Step [100/336], Loss: 524.7950\n",
      "Epoch [19/50], Step [200/336], Loss: 794.5351\n",
      "Epoch [19/50], Step [300/336], Loss: 910.2275\n",
      "18\n",
      "Epoch [20/50], Step [100/336], Loss: 657.1238\n",
      "Epoch [20/50], Step [200/336], Loss: 475.4058\n",
      "Epoch [20/50], Step [300/336], Loss: 805.6574\n",
      "19\n",
      "Epoch [21/50], Step [100/336], Loss: 617.2153\n",
      "Epoch [21/50], Step [200/336], Loss: 879.8786\n",
      "Epoch [21/50], Step [300/336], Loss: 630.0665\n",
      "20\n",
      "Epoch [22/50], Step [100/336], Loss: 741.6972\n",
      "Epoch [22/50], Step [200/336], Loss: 925.7003\n",
      "Epoch [22/50], Step [300/336], Loss: 656.5279\n",
      "21\n",
      "Epoch [23/50], Step [100/336], Loss: 723.3114\n",
      "Epoch [23/50], Step [200/336], Loss: 451.3708\n",
      "Epoch [23/50], Step [300/336], Loss: 398.1599\n",
      "22\n",
      "Epoch [24/50], Step [100/336], Loss: 469.7234\n",
      "Epoch [24/50], Step [200/336], Loss: 736.4171\n",
      "Epoch [24/50], Step [300/336], Loss: 370.7690\n",
      "23\n",
      "Epoch [25/50], Step [100/336], Loss: 583.6689\n",
      "Epoch [25/50], Step [200/336], Loss: 754.5745\n",
      "Epoch [25/50], Step [300/336], Loss: 355.2447\n",
      "24\n",
      "Epoch [26/50], Step [100/336], Loss: 802.2615\n",
      "Epoch [26/50], Step [200/336], Loss: 522.7252\n",
      "Epoch [26/50], Step [300/336], Loss: 533.6866\n",
      "25\n",
      "Epoch [27/50], Step [100/336], Loss: 940.6429\n",
      "Epoch [27/50], Step [200/336], Loss: 602.6783\n",
      "Epoch [27/50], Step [300/336], Loss: 588.0676\n",
      "26\n",
      "Epoch [28/50], Step [100/336], Loss: 464.9184\n",
      "Epoch [28/50], Step [200/336], Loss: 915.9052\n",
      "Epoch [28/50], Step [300/336], Loss: 518.9505\n",
      "27\n",
      "Epoch [29/50], Step [100/336], Loss: 699.7473\n",
      "Epoch [29/50], Step [200/336], Loss: 668.7784\n",
      "Epoch [29/50], Step [300/336], Loss: 709.6889\n",
      "28\n",
      "Epoch [30/50], Step [100/336], Loss: 765.5862\n",
      "Epoch [30/50], Step [200/336], Loss: 674.4855\n",
      "Epoch [30/50], Step [300/336], Loss: 504.6287\n",
      "29\n",
      "Epoch [31/50], Step [100/336], Loss: 601.2766\n",
      "Epoch [31/50], Step [200/336], Loss: 627.9435\n",
      "Epoch [31/50], Step [300/336], Loss: 549.2239\n",
      "30\n",
      "Epoch [32/50], Step [100/336], Loss: 830.6357\n",
      "Epoch [32/50], Step [200/336], Loss: 895.2471\n",
      "Epoch [32/50], Step [300/336], Loss: 587.2419\n",
      "31\n",
      "Epoch [33/50], Step [100/336], Loss: 795.8504\n",
      "Epoch [33/50], Step [200/336], Loss: 656.1937\n",
      "Epoch [33/50], Step [300/336], Loss: 628.0742\n",
      "32\n",
      "Epoch [34/50], Step [100/336], Loss: 609.2045\n",
      "Epoch [34/50], Step [200/336], Loss: 660.1990\n",
      "Epoch [34/50], Step [300/336], Loss: 641.1245\n",
      "33\n",
      "Epoch [35/50], Step [100/336], Loss: 680.2104\n",
      "Epoch [35/50], Step [200/336], Loss: 803.6238\n",
      "Epoch [35/50], Step [300/336], Loss: 516.8870\n",
      "34\n",
      "Epoch [36/50], Step [100/336], Loss: 437.6942\n",
      "Epoch [36/50], Step [200/336], Loss: 756.1774\n",
      "Epoch [36/50], Step [300/336], Loss: 625.4509\n",
      "35\n",
      "Epoch [37/50], Step [100/336], Loss: 375.5734\n",
      "Epoch [37/50], Step [200/336], Loss: 656.0181\n",
      "Epoch [37/50], Step [300/336], Loss: 718.7894\n",
      "36\n",
      "Epoch [38/50], Step [100/336], Loss: 684.8887\n",
      "Epoch [38/50], Step [200/336], Loss: 544.7647\n",
      "Epoch [38/50], Step [300/336], Loss: 750.2326\n",
      "37\n",
      "Epoch [39/50], Step [100/336], Loss: 615.4852\n",
      "Epoch [39/50], Step [200/336], Loss: 552.0309\n",
      "Epoch [39/50], Step [300/336], Loss: 607.3112\n",
      "38\n",
      "Epoch [40/50], Step [100/336], Loss: 601.3448\n",
      "Epoch [40/50], Step [200/336], Loss: 692.8727\n",
      "Epoch [40/50], Step [300/336], Loss: 498.6573\n",
      "39\n",
      "Epoch [41/50], Step [100/336], Loss: 973.1658\n",
      "Epoch [41/50], Step [200/336], Loss: 595.1696\n",
      "Epoch [41/50], Step [300/336], Loss: 574.8651\n",
      "40\n",
      "Epoch [42/50], Step [100/336], Loss: 677.6743\n",
      "Epoch [42/50], Step [200/336], Loss: 694.3647\n",
      "Epoch [42/50], Step [300/336], Loss: 696.3272\n",
      "41\n",
      "Epoch [43/50], Step [100/336], Loss: 660.4289\n",
      "Epoch [43/50], Step [200/336], Loss: 530.7448\n",
      "Epoch [43/50], Step [300/336], Loss: 675.8147\n",
      "42\n",
      "Epoch [44/50], Step [100/336], Loss: 767.6598\n",
      "Epoch [44/50], Step [200/336], Loss: 843.7997\n",
      "Epoch [44/50], Step [300/336], Loss: 610.8218\n",
      "43\n",
      "Epoch [45/50], Step [100/336], Loss: 763.8851\n",
      "Epoch [45/50], Step [200/336], Loss: 690.3707\n",
      "Epoch [45/50], Step [300/336], Loss: 849.5151\n",
      "44\n",
      "Epoch [46/50], Step [100/336], Loss: 780.7213\n",
      "Epoch [46/50], Step [200/336], Loss: 622.0724\n",
      "Epoch [46/50], Step [300/336], Loss: 670.4012\n",
      "45\n",
      "Epoch [47/50], Step [100/336], Loss: 577.0374\n",
      "Epoch [47/50], Step [200/336], Loss: 675.7883\n",
      "Epoch [47/50], Step [300/336], Loss: 754.3957\n",
      "46\n",
      "Epoch [48/50], Step [100/336], Loss: 728.8203\n",
      "Epoch [48/50], Step [200/336], Loss: 508.1801\n",
      "Epoch [48/50], Step [300/336], Loss: 678.2933\n",
      "47\n",
      "Epoch [49/50], Step [100/336], Loss: 618.0148\n",
      "Epoch [49/50], Step [200/336], Loss: 662.9749\n",
      "Epoch [49/50], Step [300/336], Loss: 770.0108\n",
      "48\n",
      "Epoch [50/50], Step [100/336], Loss: 513.9686\n",
      "Epoch [50/50], Step [200/336], Loss: 603.5645\n",
      "Epoch [50/50], Step [300/336], Loss: 861.0236\n",
      "49\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(N_EPOCHS):\n",
    "    for i, images in enumerate(train_set_loader):\n",
    "        output = classifer(images)\n",
    "        labels_start = i*batch_size\n",
    "        labels_end = (i+1)*batch_size\n",
    "        loss = loss_function(output, torch.stack(train_labels[labels_start:labels_end]))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (i+1) % 100 == 0:\n",
    "#             print(output)\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, N_EPOCHS, i+1, total_steps, loss.item()))\n",
    "\n",
    "#         print((i+1)*len(images), ' ', (i+1)*batch_size, ' ', (i+1)*batch_size-i*batch_size)\n",
    "    print(epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy= 11\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    correct = 0.0\n",
    "    total = 0.0\n",
    "    for i, images in enumerate(val_set_loader):\n",
    "        output = classifer(images)\n",
    "        labels_start = i*batch_size\n",
    "        labels_end = (i+1)*batch_size\n",
    "#         print(labels_start, labels_end)\n",
    "#         print(val_labels[labels_start: labels_end])\n",
    "        ground_truth = torch.stack(val_labels[labels_start: labels_end])\n",
    "        x, prediction = torch.max(output.data, 1)\n",
    "#         print(type(prediction))\n",
    "        total += batch_size\n",
    "        correct += (prediction == ground_truth).sum()\n",
    "    print(\"Accuracy= {}\".format((100*correct)/total))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.utils.data.dataloader.DataLoader'>\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
